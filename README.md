# AILRB Study - AI Literature Review Benchmark

A collaborative research study evaluating the effectiveness of AI-assisted workflows for conducting scientific literature reviews.

## Overview

This project is led by researchers from the University of Toronto, Department of Psychology, and aims to establish benchmarks for AI performance in academic research by comparing AI-only and human-AI collaborative approaches against expert-led research.

## Study Phases

1. **Problem Set Curation** (Current Phase) - Collecting expert literature reviews and evaluation rubrics
2. **Hackathon Execution** - Participants use AI tools to generate literature reviews  
3. **Quality Assessment** - Systematic evaluation of AI-generated reviews

## Getting Started

This is a simple HTML website. To view it locally:

1. Clone this repository
2. Open `index.html` in your web browser
3. Or serve it with a local web server for better performance

## Contributing

We are seeking unpublished or near-complete literature reviews from active researchers in experimental psychology, neuroscience, and related fields. Visit the website to learn more about contributing your research.

## Contact

For questions about the study, please contact: airlb.study@[yourdomain].ca

## License

Â© 2024 AI Literature Review Benchmark Study | University of Toronto
